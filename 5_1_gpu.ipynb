{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1NR4rQNSUftkRy4perE-PO2o-C9OboCib",
      "authorship_tag": "ABX9TyN48jCJRhu2JhN2wkY+m0R7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsoos/colab/blob/main/5_1_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1 - Poetry/lyrics generator\n",
        "\n",
        "*   The idea of this exercise project is to try out a few implementations of text generation AI and to compare their usefulness and discuss their limitations.\n",
        "*   In this exercise, we might need to fix some details on the code examples in the articles so that they work with Google Colab, and with the current versions of TensorFlow, PyTorch or other modules\n",
        "*   We will need kaggle.com for this exercise. If you don't have an account for it yet, create one before this exercise project (the account is free, you can use your school e-mail)\n",
        "\n"
      ],
      "metadata": {
        "id": "8VDruEtuRzUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow"
      ],
      "metadata": {
        "id": "NxGdf_bjY-Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqHeqWVfRtUh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from wordcloud import WordCloud\n",
        "from keras import regularizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/cloudcomputing2023_VincenzinaSoos/ex5/5_notebook1_gpu'\n",
        "os.chdir(folder_path)"
      ],
      "metadata": {
        "id": "LcM2TK_AZSGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the text data file\n",
        "data = open('poem.txt', encoding=\"utf8\").read()\n",
        "\n",
        "# EDA: Generating WordCloud to visualize\n",
        "# the text\n",
        "wordcloud = WordCloud(max_font_size=50,\n",
        "\t\t\t\t\tmax_words=100,\n",
        "\t\t\t\t\tbackground_color=\"white\").generate(data)\n",
        "\n",
        "# Plotting the WordCloud\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"WordCloud.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e-N25FQRYreM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the corpus by\n",
        "# splitting the text into lines\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "print(corpus[:10])"
      ],
      "metadata": {
        "id": "FXO7slfGYtUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the Tokenizer on the Corpus\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "# Vocabulary count of the corpus\n",
        "total_words = len(tokenizer.word_index)\n",
        "\n",
        "print(\"Total Words:\", total_words)"
      ],
      "metadata": {
        "id": "VW5apXGVYuVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the text into embeddings\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(tf.keras.utils.pad_sequences(input_sequences,\n",
        "\t\t\t\t\t\t\t\t\t\tmaxlen=max_sequence_len,\n",
        "\t\t\t\t\t\t\t\t\t\tpadding='pre'))\n",
        "predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "label = tf.keras.utils.to_categorical(label, num_classes=total_words+1)"
      ],
      "metadata": {
        "id": "Rhl_sT31YxKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a Bi-Directional LSTM Model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(total_words + 1, 100, input_shape=(max_sequence_len - 1,)))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150, return_sequences=True)))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.LSTM(100))\n",
        "model.add(tf.keras.layers.Dense((total_words + 1) // 2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(tf.keras.layers.Dense(total_words + 1, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "VkbE5Uy_YxHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, min_lr=1e-6)"
      ],
      "metadata": {
        "id": "kPlCky8Xz5gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(predictors, label, epochs=150, verbose=1, callbacks=[early_stopping, reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC9hMi9PYxD0",
        "outputId": "c62c2ee9-3555-4d16-c4f4-274886ddd1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m510/510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 196ms/step - accuracy: 0.0667 - loss: 6.4297 - learning_rate: 0.0010\n",
            "Epoch 2/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
            "  current = self.get_monitor_value(logs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/callback_list.py:96: UserWarning: Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss,learning_rate.\n",
            "  callback.on_epoch_end(epoch, logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m145/510\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 194ms/step - accuracy: 0.0560 - loss: 6.2643"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"The world\"\n",
        "next_words = 25\n",
        "ouptut_text = \"\"\n",
        "\n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences(\n",
        "\t\t[token_list], maxlen=max_sequence_len-1,\n",
        "\tpadding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list,\n",
        "\t\t\t\t\t\t\t\t\t\tverbose=0), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\n",
        "\tseed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ],
      "metadata": {
        "id": "2uoD08VnYw8o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}